# Theoretical resources

## Google GANs blog

https://developers.google.com/machine-learning/gan

### Summary

#### **Discriminative vs Generative Models**

The first tries to *differentiate* data samples while the second tries to generate new ones.
Formally, let $X$ be a dataset with labels $Y$. Generative models try to determine the joint probability $P(X,Y)$ while descriminative models try to understand the conditional probability $P(Y|X)$. In the second case, we see that the probabilty of $X$ is not some thing we want to capture, as $P(X,Y) = P(Y|X)*P(X)$. This means that a generator *includes the distribution of the input data itself* and that a discriminator *disregards the distribution of the input data*. Note that GANs are only one type of generative models.

Also note that this implies that the generative model requires to understand much more information than discriminative model. Indeed, it not only requires to *discriminate* the samples by finding separations, but also to assign to each separation the correct label and understand *where* or *how* these labels actually are distributed.

#### **GANs Structure**

There are 2 parts in a GAN. The first is called the **generator** and the second the **discriminator**. The first tries to generate plausible data while the second tries to distinguish real samples from those generated by the model. Both are neural networks. Basically, we start with a set of real samples and a random input. We feed the random input to the generator which generates samples of the same type as the real dataset. We then feed a mix of real and fake images to the disciminator and let him predict whether they are real of generated. At the end, we have 2 losses, one for the generator and one for the discriminator.

#### **Discriminator**

The discriminator is simple to train as it is trained as a simple classifier that has 2 labels: real/fake. While the discriminator is training, we ignore the generator loss.

#### **Generator**

The generator takes as input a random element as described earlier (which is generally composed of noise), and generated a data sample before giving it to the discriminator. Its loss is based on its ability to fool the discriminator. Note that the backpropagation goes through the disciminator since its loss depends on it but we don't want the discriminator to modify on it, so while we backpropagate for the generator, we consider the discriminator as a constant function and freeze it.

#### **Training**

To train the GAN, we alternativelly train every model for one or more epoch. Note that this alternating training raises some convergence issues where it can become hard for both of the models to converge. The problem is that while the discriminator has 100% accuracy, the generator has none and while the generator has 100% accuracy, the discriminator has at most 50%. Basically the discriminator feedback becomes less meaningful over time and if the generator keeps training after the discciminator having passed the point where it gives random predictions, then the genrator trains on junk feedback. Convergence is often unstable, hard to grasp state.

#### **Loss Functions**

Since we want to approximate the joint disribution of the data, the loss should reflect that. To this regard, we have 2 losses proposed here; the minimnax loss, present in the original GANs paper and the Wasserstein loss, in the corresponding paper. The common way of defining both the losses is to define it by using terms that are influenced by either of the two models. 

The **Minimax loss** is defined as 

$$
E_x[log(D(x))] + E_z[log(1 - D(G(z)))]
$$

where for $x$ being a real data input for the discriminator, $D(.)$ is the discriminator probability that the input is real, $G(.)$ the generated data sample from noise. Thus $E_x$ is the expected value over all real data instances while $E_z$ the expected value over the noise distribution. The formula is simply a binary cross-entropy with negative samples depending on a generator. The discriminator tries to maximize is while the generator tries to minimize it. An example of implementation can be found here: https://github.com/tensorflow/gan/blob/master/tensorflow_gan/python/losses/losses_impl.py. Note that this loss is know for getting stuck in the early stages of the training and 

The **Wasserstein** loss on the other hand modifies the model in such a way that the descriminator outputs a number instead of a probability, and tries to make it bigger for real instances than for fake instances. In this setup, as it doesn't really discriminate, it is called a *critic*.
The loss formulation are simple. The critic loss is:

$$
D(x) - D(G(z))
$$

while the generator loss is given by:

$$
D(G(z))
$$

It is based on the Wasserstein distance (https://en.wikipedia.org/wiki/Wasserstein_metric) between distributions, here the generated and real distribution, or earth mover's distance (https://en.wikipedia.org/wiki/Earth_mover's_distance).
The generator tries to maximize the second.

#### **Common Problems**

If the discriminator is too good, the backpropagation may fail due to the **vanishing gradient**. Moreover, (and much like reinforcement learning), if the generator find a sample that works, it may be tempted to always output the same and not care about the input noise. The discriminator will then likely automatically learn to always reject the generated sample. If the discriminator happens to get stuck in a local minima at that point, then the generator may cirle around several such solutions. This problem is refered as **mode collapse**. Solutions include the Wasserstein loss as well as unrolled GANs https://arxiv.org/pdf/1611.02163.pdf.

#### More to GANs:
- progressive GANs
- Conditional GANs
- Image-to-Image Translation
- CycleGAN
- Text-toImage Synthesis
- Super-resolution
- Face Inpainting
- Text-to-Speech

## GANs (original paper, 2014) 

https://arxiv.org/abs/1406.2661

The described idea is a general vision of GANs. We suppose $D$ and $G$ are multi-layered perceptrons and let $p_g$ be the generator distribution over the data $x$. Let also $p_z(z)$ be a *prior* over the input noise variables. We can then define the generator as a mapping from noise to the data space $G(z;\theta_g) : S_z \rarr S_x$. Of course $G$ is differentiable with respect to $\theta_g$. We finally define $D(x;\theta_d)$ a multilayered perceptron outputing a single scalar, representing the probability that $x$ came from real data and not $p_g$. 

We want $D$ to maximize its ability to predict correctly while training $G$ to minimize $log(1 - D(G(z)))$ (same as minimizing $D(G(z))). In other words, the objective function looks like this:
$$
\min_G{\max_D{V(D,G)}}
$$
where
$$
V(D,G) = E_{x \sim p_{data}(x)}[log(D(x))] + E_{z \sim p_z(x)}[log(1 - D(G(z)))]
$$

The paper suggests to fully train $D$ at each iteration while performing a single step for $G$ to always let $D$ be in its optimal solution. Moreover they say in early stages, $log(1 - D(G(z)))$ is near 0 as the discriminator confidence that the data is generated, namely $D(G(z))$ is close to 1. This implies that the gradient will be very small. Note that this direclty comes from the fact that the two models have opposing losses, and since the backprop goes trough $D$ before going through $G$, *the norm of the backpropagating gradient applied on G also depends on how well D performs*. This is why they propose to replace the loss by the maximization of $log(D(G(z)))$ with respect to $G$ in the early stage of computation.

The algorithm is fairly simple 

- $\forall e \in [E]$ do
    - for $k$ steps do
        - sample batches of $m$ noise samples $\{z^{(1)}, ..., z^{(m)}\} \sim p_g(z)$
        - sample batches of $m$ samples $\{x^{(1)}, ..., x^{(m)}\} \sim p_{data}(x)$
        - perform stochastic gradient ascend wrt $\theta_d$ over
        
            $\nabla_{\theta_d} \frac{1}{m} \sum_{i=1}^m [log(D(x^{(i)})) + log(1 - D(G(z^{(i)})))]$

    - sample batches of $m$ noise samples $\{z^{(1)}, ..., z^{(m)}\} \sim p_g(z)$
    - perform stochastic gradient descent wrt $\theta_g$ over

        $\nabla_{\theta_g} \frac{1}{m} \sum_{i=1}^m [log(1 - D(G(z^{(i)})))]$

Something that is not mentioned in the paper up to this point is whether they train a regular sample a the same time as they train a generated sample for the discriminator. So any real image is coupled with a generated image? Or is it a simplification due to the batch? Do they sample elements from $G$ and then train a binary classifier normaly by merging the real and generated data or always give 2 samples at a time? Must the real data and generated data batches have the same length?

Theretically, the optimal discriminator is the one that corresponds to $D^*_G(x) = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)}$, which given a sample x, will say it is real if it is more probable to be given by the real data distribution rather than the generator distribution (which makes sense).

## Wasserstein (2017)

https://arxiv.org/abs/1701.07875

### Summary

The paper generalizes to the problem of unsupervised learning, namely how and what does it mean to learn a distribution? The general answer is that to consider the minimum likelihood approach. Given a parametric identity family $(P_\theta)_{\theta \in \mathbf{R}^d}$, we want the one that minimizes the likelihood of our data samples $\{x^{(i)}\}_{i=1}^m$: 
$$\max_{\theta \in \mathbf{R}^d}{ \frac{1}{m} \sum_{i=1}^m log(P_{\theta} (x^{(i)}) )}$$
Given the real data distribution $\mathbf{P}_r$, this is equivalent to minimizing $KL(\mathbf{P}_r || \mathbf{P}_\theta)$. In practice however, the approximation of $\mathbf{P}_r$ may not exist in the given dimentional space, resulting in a negligeable intersection between $\mathbf{P}_\theta$ and $\mathbf{P}_r$ implying that the KL divergence is not defined.
RECALL: $$KL(P||Q) = \sum_{x \in \mathcal{X}} P(x) log(\frac{P(x)}{Q(x)})$$
We thus change the formulation to the following. Let $Z \sim p(z)$ be a random variable and let it pass through a parametric function $g_\theta : Z \rarr \mathcal{X}$. 

We want to find a good way to estimate how close the estimated and the true distribution are to each other, or in other words, to find a good distance function 
$\rho (\mathbf{P}_\theta, \mathbf{P}_r)$. A main criteria for $\rho$ is the convergence of the sequence. We say that a sequence of distributions $(\mathbf{P}_t)_{t \in \mathcal{N}}$ converges iff $\exists \ \mathbf{P}_\infty$ such that $\rho (\mathbf{P}_t, \mathbf{P}_\infty) \rarr 0$ and in fact depends on the definition of $\rho$. The definition of $\rho$ implies a tradeoff between the stength of its topology and the ease of convergence (if the convergence criteria is weaker, so is the topology, meaning more function series will converge).

To define our mapping $\theta \rarr \mathbf{P}_\theta$, we need it to be continous (namely $\mathbf{P}_{\theta_t} \rarr \mathbf{P}_{\theta}$ for $\theta_t \rarr \theta$) but this also depends on the distance definition between $\mathbf{P}_{\theta_t}$ and $\mathbf{P}_\theta$. By assuming $\mathbf{P}_\theta$ is continuous, we can say that with a continuous $\rho$ we have a continuous loss function $\theta \rarr \rho (\mathbf{P}_\theta, \mathbf{P}_\infty)$.

### Distances

Let $\mathcal{X}$ be a compact metric set (such as the space of images [0, 1]d) and let $\Sigma$ denote the set of all the Borel subsets of $\mathcal{X}$. Let $Prob(\mathcal{X})$ be the space of probabilities measures defined on $\mathcal{X}$. We can define the following distances:

- Total Variation (TV)

    $ \delta(\mathbf{P}_r, \mathbf{P}_g) = \sup_{A \in \Sigma} |\mathbf{P}_r(A) - \mathbf{P}_g(A)|$

- Kullback-Liebler (KL)

    $KL(\mathbf{P}_r||\mathbf{P}_g) = \int \mathbf{P}_r(x) \log(\frac{\mathbf{P}_r(x)}{\mathbf{P}_g(x)}) d\mu (x)$

- Jensen-Shannon (JS)

    $JS(\mathbf{P}_r||\mathbf{P}_g) = KL(\mathbf{P}_r ||\mathbf{P}_m) + KL(\mathbf{P}_g||\mathbf{P}_m)$

    where $\mathbf{P}_m = \frac{1}{2}(\mathbf{P}_r + \mathbf{P}_g)$

- Earth-Mover (EM)

    $W(\mathbf{P}_r, \mathbf{P}_g) = \inf_{\gamma \in \Pi(\mathbf{P}_r, \mathbf{P}_g)} E_{(x,y) \sim \gamma}[||x-y||]$

    where $\Pi(\mathbf{P}_r, \mathbf{P}_g)$ denotes the set of all joint distributions $\gamma (x, y)$ whose marginals are respectively $\mathbf{P}_r$ and $\mathbf{P}_g$.

From what I understood from the examples given on the paper, in the case of a *translated* distribution, that is more or less at every point distant from the target distribution, the EM distance gives a better measure than the others, that either give a worst case scenario uneffiscient in high dimension (TV) or a measure of divergence that is only relevant if the two distributions are somewhat similar but diverge at some point (KL & JS). Moreover, with EM, the two distributions can have an intersection set of zero measure unlike the other distributions, which will always yield their results otherwise.

### From Theoretical to Practical Use

The proof that the loss is related to the optimization of 
$$
\max_{w \in W} E_{x \sim \mathbf{P}_r}[f_x(x)] - E_{z \sim p(z)}[f_w(g_\theta(z))]
$$
requires some Lipschitz contraint which they circumvent by clamping the weights at every iteration. They themselves say it is a terrible idea but couldn't find anything else. The overall algorithm is finally given by:

- init 
    - lr = $\alpha$
    - clipping = $c$
    - batch_size = $m$
    - num of critic iter per gen iter = $n_{critic}$
    - init generator $\theta_0$
    - init critic $w_0$

    (in practice: $\alpha = 5*10^{-5}, c = 0.01, m = 64, n_{critic} = 5$)

- while $\theta$ didn't converge do
    - $\forall t \in [n_{critic}]$ do
        - Sample $\{x^{(i)}\}_{i=1}^m \sim \mathbf{P}_r$
        - Sample $\{z^{(i)}\}_{i=1}^m \sim p(z)$
        - $grad_w \larr \frac{1}{m} \nabla_w \big[\sum_{i=1}^m f_w(x^{(i)}) - \sum_{i=1}^m f_w(g_\theta(z^{(i)}))\big]$
        - $w \larr w + \alpha RMSProp(w, grad_w)$
        - $w \larr clip(w, -c, c)$

    - Sample $\{z^{(i)}\}_{i=1}^m \sim p(z)$
    - $grad_\theta \larr - \frac{1}{m} \nabla_\theta \sum_{i=1}^m f_w(g_\theta(z^{(i)}))]$
    - $\theta \larr \theta - \alpha RMSProp(\theta, grad_\theta)$

Note that this form implies that we should train the critic **optimaly** at every iteration as we copped with the vanishing gradient problem.

## DCGANs (Deep Convolutional Generative Adversarial Networks):

https://arxiv.org/pdf/1511.06434.pdf

### Summary

They present the so-called DCGANs (Deep Convolutional Generative Adversarial Networks) architecture.

#### Stability

There were problems with stability with CNNs when using GANs. In the paper they say that the following cope with this problem:
- replace pooling layers with *strided convolutions* for the discriminator and with *fractional-strided convolutions* for the generator. -> Need to check this out
- Use batch norm (0 mean & unit variance)
- Remove Fully connected layers 
- For the generator: use ReLU for all layers on except for the last, where should use Tanh
- For the discriminator: use the LeakyReLU for all layers

#### Preprocessing, Hyperparamters and Optimizer

- No preprocessing except mapping images into [-1, 1]
- batches of 128
- weights init with 0 mean, 0.02 std
- slope of LeakyReLU: 0.2
- optimizer: Adam
- learning rate: 0.0002
- momentum $\beta_1$: 0.5

## NIPS 2016 Tutorial: Generative Adversarial Networks

Instead of seeing the GAN problem as an optimization problem, it is better to see it as a *2 player game*. This means that we have 2 optimization functions $J^{D}(\bold \theta_D, \bold \theta_G)$ where $D$ is the discriminator function and can only control $\theta_D$ and $J^{G}(\bold \theta_D, \bold \theta_G)$ where $G$ is the Generator function and can only control $\theta_G$.

In this setup, the equilibrium point is refered as the *Nash equilibrium*, which the tuple $(\theta_D, \theta_G)$ minimizing $J^{D}$ wrt $\theta_D$ as well as $J^{G}$ wrt $\theta_G$. $G$ is a function that given a sample $z$ sampled from a prior disctribution will return a sample $\bold x$ drawn from $p_{model}$ (I think they meant drawn from the support of the distribution).

## Dataset 

https://www.kaggle.com/greatgamedota/ffhq-face-data-set

## Ideas

What if we train the generator to have a regularization loss over an input and output to make it *modify* real images?

Is there a way to determine, going through the network reversely, whether a sample is actally part of the distribution or not, given $x$: $P(z | G(z) = x)$ for $z$ part of the noise?

Since the deconvolution is the inverse of a convolution, can we run the generator backward to see the initial seed or can we run the discriminator backward to see what it would output?

